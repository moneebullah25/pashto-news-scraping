# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18-wy3Vpkp-sJuQKrTZPOtp-q5AnT0dXh
"""

!pip install bs4

from bs4 import BeautifulSoup
import urllib.request
import re
import time
from google.colab import files

def get_data(url):
    data = urllib.request.urlopen(url).read()
    page_content = data.decode("utf-8")
    soup = BeautifulSoup(page_content, "html.parser")
    return soup

# Main link for list of news links
#soup = get_data("https://khybernews.tv/pu/%d8%ae%db%90%d8%a8%d8%b1%d9%be%da%9a%d8%aa%d9%88%d9%86%d8%ae%d9%88%d8%a7/") # KPK p1
#soup = get_data("https://khybernews.tv/pu/%d9%82%d8%a7%d9%85%d9%89-%d8%ae%d8%a8%d8%b1%d9%88%d9%86%d9%87/") # Komi Khabrona p1
#soup = get_data("https://khybernews.tv/pu/%d8%a8%d9%84%d9%88%da%86%d8%b3%d8%aa%d8%a7%d9%86/") # Balochistan p1
#soup = get_data("https://khybernews.tv/pu/%d9%82%d8%a7%d9%85%d9%89-%d8%ae%d8%a8%d8%b1%d9%88%d9%86%d9%87/page/1440/") #  Komi Khabrona p1440 
#soup = get_data("https://khybernews.tv/pu/%d8%a7%d9%81%d8%ba%d8%a7%d9%86%d8%b3%d8%aa%d8%a7%d9%86/") # Afghanistan p1
#soup = get_data("https://khybernews.tv/pu/%d8%a7%d9%81%d8%ba%d8%a7%d9%86%d8%b3%d8%aa%d8%a7%d9%86/page/2/") # Afghanistan p2
soup = get_data("https://khybernews.tv/pu/%d8%a8%d9%84%d9%88%da%86%d8%b3%d8%aa%d8%a7%d9%86/page/2/") # Balochistan p2

print(soup.prettify())

def get_relevant_links(soup):
  lista = []
  items = soup.find_all('h3', class_="entry-title td-module-title")
  if items == None:
    return
  for item in items:
    lista.append(item.find('a').get('href'))
  
  return lista

links = get_relevant_links(soup)
print(len(links), links)

def extract_pashto(string):
  pashto = ''''''
  for char in string:
    if re.search(r"[\u0080-\uFFFF]", char):
      pashto = pashto + char
      print(char, end="")
    if char == " ":
      pashto = pashto + " "
    if char == "۔":
      pashto = pashto + char

    
  return pashto

def extract_p_from_links(links):
  pashto = u''''''
  for link in links:
    soup = get_data(link)
    if soup != None:
      par = soup.find('div', class_="td-post-content tagdiv-type")  

      if par is not None:
        par = str(par)

        for char in par:
          if re.search(r"[\u0080-\uFFFF]", char) != None:
            pashto = pashto + char
          if char == " ":
            pashto = pashto + " "
          if char == "۔":
            pashto = pashto + char

        print(pashto)


  return pashto

string = extract_p_from_links(links)

print(string)

print(string)

string = '''<div class="td-post-content tagdiv-type">
<!-- image --><div class="td-post-featured-image"><a data-caption="" href="https://khybernews.tv/pu/wp-content/uploads/2021/11/750780_6232773_President-for_akhbar.jpg"><img alt="" class="entry-thumb td-modal-image" height="398" sizes="(max-width: 696px) 100vw, 696px" src="https://khybernews.tv/pu/wp-content/uploads/2021/11/750780_6232773_President-for_akhbar-696x398.jpg" srcset="https://khybernews.tv/pu/wp-content/uploads/2021/11/750780_6232773_President-for_akhbar-696x398.jpg 696w, https://khybernews.tv/pu/wp-content/uploads/2021/11/750780_6232773_President-for_akhbar-300x171.jpg 300w, https://khybernews.tv/pu/wp-content/uploads/2021/11/750780_6232773_President-for_akhbar.jpg 700w" title="750780_6232773_President-for_akhbar" width="696"/></a></div>
<!-- content --><p>صدر پاکستان ډاکټر عارف علوې په دوه ورځنې سفر کوئټې ته  راورسېد. لومړۍ مېرمن بېګم ثمینه علوي هم له صدر پاکستان سره ده.</p>
<p>صدر پاکستان ډاکټر عارف علوې په دوه ورځني سفر کوېټې ته راورسېد. د کوټې په ائيرپورټ باندې د رسېدو پر مهال بلوچستان ګورنر، وزيراعلي او نورو اعٰلې چارواکو د صدر پاکستان ته هرکلى وکړ.لومړۍ مېرمن بېګم ثمینه علوي هم له صدر پاکستان سره ده ، ګورنر هاؤس ته په رسېدو د بلوچستان کانسټيبلري ځوانانو و صدر پاکستان ته سلامي وركړه، صدر پاکستان ډاکټر عارف علوې ګورنر بلوچستان او وزير اعلې بلوچستان سره په ملاقات کې پر موجوده سياسې او انتظامې امور باندې خبرې اترې وکړې ، کوټې ته د خپل سفر په مهال به صدر ډاکټر عارف علوې به نن په ګورنر هاؤس کې سياسې مشرانو او قبائلې عمائدينو ته وینا کوې ،</p>
</div>'''

def extract_all_p(string):
  pashto = ''''''
  for char in string:
    if re.search(r"[\u0080-\uFFFF]", char):
      pashto = pashto + char
    if char == " ":
      pashto = pashto + " "
    if char == "۔":
      pashto = pashto + char
  return pashto


def ex_p(string):
  m_p = []

  if re.search("<p>", string) != None:
    m_p.append(re.search("<p>", string).span()[1] + 1)
  else:
    return

  if len(m_p) == 0:
    return

  index = 0
  while (re.search("<p>", string[m_p[index]]) != None):
    m_p.append(m_p[index] + re.search("<p>", string[m_p[index]:]).span()[1] + 1)

  m_ep = []
  m_ep.append(re.search("<\p>", string).span()[1] + 1)
  
  if len(m_ep) != len(m_ep):
    return

  index = 0
  while (re.search("<\p>", string[m_ep[index]]) != None):
    m_ep.append(m_ep[index] + re.search("<\p>", string[m_ep[index]:]).span()[1] + 1)


  for x, y in m_p, m_ep:
    print(string[x : y])


#ex_p(string)

string.


print(re.findall(r'\\u(.){4}', string))

m_p = re.search("<p>", string)
m_p2 = re.search("<p>", string[m_p.span()[1]: ])

print(string)

m_ep = re.search("<\p>", string)
m_ep2 = re.search("<\p>", string[m_ep.span()[1]: ])

pashto = re.findall(r'<p>\s+', string)
#print(pashto)

print(m_p.span(), m_p2.span())
print(m_ep.span())


print(string[m_p.span()[0] : m_ep.span()[0]])
print(string[m_p.span()[1] : m_ep.span()[1]])

def extract_all_p(string):
  pashto = ''''''
  for char in string:
    if re.search(r"[\u0080-\uFFFF]", char):
      pashto = pashto + char
    if char == " ":
      pashto = pashto + " "
    if char == "۔":
      pashto = pashto + char
  return pashto


def save_to_file(string, filename):
  from google.colab import files

  with open(filename, 'w') as f:
    f.write(string)

data = extract_all_p(string)
print(data)
save_to_file(data)

files.download('example.txt')

from bs4 import BeautifulSoup
import urllib.request
import re
import time
from google.colab import files

def get_data(url):
    data = urllib.request.urlopen(url).read()
    page_content = data.decode("utf-8")
    soup = BeautifulSoup(page_content, "html.parser")
    return soup

def get_relevant_links(soup):
  lista = []
  items = soup.find_all('h3', class_="entry-title td-module-title")
  if items == None:
    return
  for item in items:
    lista.append(item.find('a').get('href'))
  
  return lista

def extract_pashto(string):
  pashto = ''''''
  res = re.sub(' +', ' ', string)
  for char in res:
    if re.search(r"[\u0080-\uFFFF]", char):
      pashto = pashto + char
    if char == " ":
      pashto = pashto + " "
    if char == "۔":
      pashto = pashto + char

  return pashto
  

def extract_pashto_from_links(links):
  pashto = ''''''
  for link in links:
    soup = get_data(link)
    if soup != None:
      par = soup.find('div', class_="td-post-content tagdiv-type")
      if par != None:
        data = extract_pashto(str(par))
        if data != None and len(data) > 0: 
          pashto += data

  return pashto

def save_to_file(string, filename):

  with open(filename, 'w') as f:
    f.write(string)

#soup = get_data("https://khybernews.tv/pu/%d8%ae%db%90%d8%a8%d8%b1%d9%be%da%9a%d8%aa%d9%88%d9%86%d8%ae%d9%88%d8%a7/") # KPK p1
#soup = get_data("https://khybernews.tv/pu/%d9%82%d8%a7%d9%85%d9%89-%d8%ae%d8%a8%d8%b1%d9%88%d9%86%d9%87/") # Komi Khabrona p1
#soup = get_data("https://khybernews.tv/pu/%d8%a8%d9%84%d9%88%da%86%d8%b3%d8%aa%d8%a7%d9%86/") # Balochistan p1
#soup = get_data("https://khybernews.tv/pu/%d9%82%d8%a7%d9%85%d9%89-%d8%ae%d8%a8%d8%b1%d9%88%d9%86%d9%87/page/1440/") #  Komi Khabrona p1440 
#soup = get_data("https://khybernews.tv/pu/%d8%a7%d9%81%d8%ba%d8%a7%d9%86%d8%b3%d8%aa%d8%a7%d9%86/") # Afghanistan p1
#soup = get_data("https://khybernews.tv/pu/%d8%a7%d9%81%d8%ba%d8%a7%d9%86%d8%b3%d8%aa%d8%a7%d9%86/page/2/") # Afghanistan p2
#soup = get_data("https://khybernews.tv/pu/%d8%a8%d9%84%d9%88%da%86%d8%b3%d8%aa%d8%a7%d9%86/page/2/") # Balochistan p2

def get_news(URL, st, end):
  start = st
  
  while start < end:
    # Selection file name based on URL
    url = URL + "/page/" + str(start) + "/"
    filename = ""
    if URL == "https://khybernews.tv/pu/%d8%a8%d9%84%d9%88%da%86%d8%b3%d8%aa%d8%a7%d9%86/":
      filename = filename + "Balochistan-p" + str(start) + ".txt"
    elif URL == "https://khybernews.tv/pu/%d8%a7%d9%81%d8%ba%d8%a7%d9%86%d8%b3%d8%aa%d8%a7%d9%86/":
      filename = filename + "Afghanistan-p" + str(start) + ".txt"
    elif URL == "https://khybernews.tv/pu/%d9%82%d8%a7%d9%85%d9%89-%d8%ae%d8%a8%d8%b1%d9%88%d9%86%d9%87/":
      filename = filename + "NationalNews-p" + str(start) + ".txt"
    elif URL == "https://khybernews.tv/pu/%d8%ae%db%90%d8%a8%d8%b1%d9%be%da%9a%d8%aa%d9%88%d9%86%d8%ae%d9%88%d8%a7/":
      filename = filename + "KPK-p" + str(start) + ".txt"
    else:
      filename = filename + "Others-p" + str(start) + ".txt"

    #Initializing Variables
    pashto = ''''''
    links = []

    #Processing Data
    soup = get_data(url)
    links = get_relevant_links(soup)
    pashto = extract_pashto_from_links(links)
    save_to_file(pashto, filename)

    #Downloading File
    files.download(filename)

    #Updating Variables
    start += 1

balochistan = "https://khybernews.tv/pu/%d8%a8%d9%84%d9%88%da%86%d8%b3%d8%aa%d8%a7%d9%86/"
afghanistan = "https://khybernews.tv/pu/%d8%a7%d9%81%d8%ba%d8%a7%d9%86%d8%b3%d8%aa%d8%a7%d9%86/"
nationalNews = "https://khybernews.tv/pu/%d9%82%d8%a7%d9%85%d9%89-%d8%ae%d8%a8%d8%b1%d9%88%d9%86%d9%87/"
kpkNews = "https://khybernews.tv/pu/%d8%ae%db%90%d8%a8%d8%b1%d9%be%da%9a%d8%aa%d9%88%d9%86%d8%ae%d9%88%d8%a7/"

get_news(balochistan, 1, 5) # total 155
get_news(afghanistan, 1, 5) # total 249
get_news(nationalNews, 1, 5) # total 1441
get_news(kpkNews, 1, 5) # total 1090







